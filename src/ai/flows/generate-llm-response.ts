// This is a Genkit flow that takes a query as input and returns a relevant response generated by an LLM.
'use server';
/**
 * @fileOverview A flow to generate a response from an LLM based on a query.
 *
 * - generateLLMResponse - A function that handles the generation of the LLM response.
 * - GenerateLLMResponseInput - The input type for the generateLLMResponse function.
 * - GenerateLLMResponseOutput - The return type for the generateLLMResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const GenerateLLMResponseInputSchema = z.object({
  query: z.string().describe('The query to be answered by the LLM.'),
});
export type GenerateLLMResponseInput = z.infer<typeof GenerateLLMResponseInputSchema>;

const GenerateLLMResponseOutputSchema = z.object({
  response: z.string().describe('The response generated by the LLM.'),
  isAnswerable: z.boolean().describe('Indicates whether the LLM can answer the query.'),
});
export type GenerateLLMResponseOutput = z.infer<typeof GenerateLLMResponseOutputSchema>;

export async function generateLLMResponse(input: GenerateLLMResponseInput): Promise<GenerateLLMResponseOutput> {
  return generateLLMResponseFlow(input);
}

const prompt = ai.definePrompt({
  name: 'generateLLMResponsePrompt',
  input: {schema: GenerateLLMResponseInputSchema},
  output: {schema: GenerateLLMResponseOutputSchema},
  prompt: `You are a helpful AI assistant. You will answer the user's query if possible.
If the question is unanswerable respond with isAnswerable=false and an empty response.
If you are able to answer, respond with isAnswerable=true and the answer in the response field.

Query: {{{query}}}`,
});

const generateLLMResponseFlow = ai.defineFlow(
  {
    name: 'generateLLMResponseFlow',
    inputSchema: GenerateLLMResponseInputSchema,
    outputSchema: GenerateLLMResponseOutputSchema,
  },
  async input => {
    const {output} = await prompt(input);
    return output!;
  }
);
